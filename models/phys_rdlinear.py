import torchimport torch.nn as nnfrom models.layers import SeriesDecompclass FreqRevIN(nn.Module):    # ... (保持原有的频域归一化代码不变) ...    def __init__(self, num_features: int, eps=1e-5, affine=True):        super().__init__()        self.num_features = num_features        self.eps = eps        self.affine = affine        if self.affine:            self.affine_weight = nn.Parameter(torch.ones(1, num_features, 1))            self.affine_bias = nn.Parameter(torch.zeros(1, num_features, 1))    def forward(self, x, mode='norm'):        if mode == 'norm':            self.mean = torch.mean(x, dim=1, keepdim=True).detach()            self.stdev = torch.sqrt(torch.var(x, dim=1, keepdim=True, unbiased=False) + self.eps).detach()            x = (x - self.mean) / self.stdev            if self.affine:                x = x * self.affine_weight + self.affine_bias            return x        return xclass PGFA(nn.Module):    # ... (保持 PGFA 代码不变，它是基于 Speed Hz 的硬物理约束) ...    def __init__(self, freq_dim: int, max_freq: float = 512.0, init_sigma: float = 2.0):        super().__init__()        self.sigma = nn.Parameter(torch.tensor(init_sigma))        self.alpha = nn.Parameter(torch.tensor(0.5))        self.register_buffer('freq_axis', torch.linspace(0, max_freq, freq_dim).view(1, -1, 1))    def forward(self, seasonal: torch.Tensor, speed_hz: torch.Tensor) -> torch.Tensor:        s = speed_hz.view(-1, 1, 1)        f = self.freq_axis        sig = torch.clamp(self.sigma, min=0.5, max=10.0)        denom = 2 * (sig ** 2)        # 基频 + 2/3次谐波 + 0.5次次谐波        mask = torch.exp(- (f - s) ** 2 / denom) + \               torch.exp(- (f - 2 * s) ** 2 / denom) + \               torch.exp(- (f - 3 * s) ** 2 / denom) + \               0.5 * torch.exp(- (f - 0.5 * s) ** 2 / denom)        return seasonal * (1 + self.alpha * mask)class AdaptiveFusion(nn.Module):    def __init__(self, hidden_dim):        super().__init__()        # 融合网络输入：[Speed_Hz, Current_RMS_Norm] -> 2维        # 输出：3个模态的注意力权重        self.weight_net = nn.Sequential(            nn.Linear(2, 16),            nn.ReLU(),            nn.Linear(16, 3)  # [w_vib, w_audio, w_current]        )        self.softmax = nn.Softmax(dim=1)    def forward(self, h_vib, h_aud, h_cur, speed, load_proxy):        # speed: [B, 1] (Hz)        # load_proxy: [B, 1] (Normalized Current RMS)        # 为了网络好训练，建议把 speed 也简单归一化一下输入给 MLP        # 假设最大转速 60Hz，除以 100 即可        cov = torch.cat([speed / 100.0, load_proxy], dim=1)  # [B, 2]        weights = self.softmax(self.weight_net(cov))        # 加权融合        fused = weights[:, 0:1] * h_vib + \                weights[:, 1:2] * h_aud + \                weights[:, 2:3] * h_cur        return fused, weightsclass PhysRDLinearCls(nn.Module):    def __init__(self, config, enable_pgfa=True, enable_mtl=False):        """        Args:            enable_mtl: 虽然保留参数名兼容，但内部实现已去除 Load Regression Head。                        我们采用 "Input-Driven" 策略，Current RMS 直接作为输入。        """        super().__init__()        self.enable_pgfa = enable_pgfa        # === 1. Vibration Stream (Micro: Low Freq) ===        self.revin_micro = FreqRevIN(config.FREQ_DIM, affine=False)        self.decomp = SeriesDecomp(kernel_size=25)        if self.enable_pgfa:            self.pgfa = PGFA(config.FREQ_DIM, max_freq=config.SAMPLE_RATE / 2, init_sigma=config.PGFA_SIGMA)        self.feat_vib = nn.Linear(config.FREQ_DIM, config.HIDDEN_DIM)        # === 2. Macro Stream (High Freq Supplement) ===        self.revin_macro = FreqRevIN(config.FREQ_DIM, affine=False)        self.feat_macro = nn.Linear(config.FREQ_DIM, config.HIDDEN_DIM)        self.vib_proj = nn.Linear(config.HIDDEN_DIM * 2, config.HIDDEN_DIM)        # === 3. Acoustic Stream ===        self.feat_aud = nn.Sequential(            nn.Linear(config.AUDIO_DIM, 64),            nn.ReLU(),            nn.Linear(64, config.HIDDEN_DIM)        )        # === 4. Current Spectrum Stream (For Electrical Faults) ===        self.feat_cur = nn.Sequential(            nn.Linear(config.CURRENT_DIM, 64),            nn.ReLU(),            nn.Linear(64, config.HIDDEN_DIM)        )        # === 5. Trend Branch (Soft Sensing Driven) ===        # 输入: Trend_Component(512) + Speed(1) + Current_RMS(1)        # 物理意义: 利用 Current RMS 代理负载，直接学习 Baseline Energy        self.trend_proj = nn.Sequential(            nn.Linear(config.FREQ_DIM + 2, config.HIDDEN_DIM),            nn.ReLU(),            nn.Dropout(0.3)        )        # === 6. Fusion & Classifier ===        self.fusion_layer = AdaptiveFusion(config.HIDDEN_DIM)        self.cls_head = nn.Sequential(            nn.BatchNorm1d(config.HIDDEN_DIM * 2),  # Norm Fusion + Trend            nn.Linear(config.HIDDEN_DIM * 2, config.HIDDEN_DIM),            nn.ReLU(),            nn.Dropout(0.5),            nn.Linear(config.HIDDEN_DIM, config.NUM_CLASSES)        )    def forward(self, micro, macro, acoustic, current_spec, speed, load_proxy):        """        params:            load_proxy: [B, 1] Normalized Current RMS (Soft Sensor)        """        # --- A. Vibration (Decomp + PGFA) ---        mic_norm = self.revin_micro(micro, 'norm')        sea_mic, trend_mic = self.decomp(mic_norm)  # Trend [B, F, 1]        if self.enable_pgfa:            sea_mic = self.pgfa(sea_mic, speed)        h_sea_mic = self.feat_vib(sea_mic.squeeze(-1))        # Macro        mac_norm = self.revin_macro(macro, 'norm')        h_mac = self.feat_macro(mac_norm.squeeze(-1))        h_vib = self.vib_proj(torch.cat([h_sea_mic, h_mac], dim=1))        # --- B. Other Modalities ---        h_aud = self.feat_aud(acoustic)        h_cur = self.feat_cur(current_spec)  # Current Spectrum for Faults        # --- C. Adaptive Fusion ---        # 权重由 Speed 和 Current RMS (Load Proxy) 共同决定        h_seasonal, weights = self.fusion_layer(h_vib, h_aud, h_cur, speed, load_proxy)        # --- D. Trend Branch (Soft Baseline) ---        # 将物理协变量注入 Trend 分支，学习随工况变化的能量基线        trend_flat = trend_mic.squeeze(-1)        # Speed 简单归一化 /100 保持数值稳定        trend_input = torch.cat([trend_flat, speed / 100.0, load_proxy], dim=1)        h_trend = self.trend_proj(trend_input)        # --- E. Final Output ---        out_feat = torch.cat([h_trend, h_seasonal], dim=1)        logits = self.cls_head(out_feat)        # 返回 None 作为第二个参数，告诉 Trainer 不需要计算回归 Loss        return logits, None