import torchimport torch.nn as nnfrom models.layers import SeriesDecompclass FreqRevIN(nn.Module):    """    [核心修复] 频域可逆实例归一化 (Frequency-domain RevIN)    替代 BatchNorm1d。    作用：对每个样本的频谱单独进行 Z-Score 归一化，消除不同负载下的能量（幅值）差异。    逻辑：(x - mean(x)) / std(x)，针对 dimension=1 (Freq) 操作。    """    def __init__(self, num_features: int, eps=1e-5, affine=True):        super().__init__()        self.num_features = num_features        self.eps = eps        self.affine = affine        if self.affine:            self.affine_weight = nn.Parameter(torch.ones(1, num_features, 1))            self.affine_bias = nn.Parameter(torch.zeros(1, num_features, 1))    def forward(self, x, mode='norm'):        # x shape: [Batch, Freq, 1]        if mode == 'norm':            # 1. 计算每个样本在频域上的均值和标准差 (Instance Stats)            self.mean = torch.mean(x, dim=1, keepdim=True).detach()            self.stdev = torch.sqrt(torch.var(x, dim=1, keepdim=True, unbiased=False) + self.eps).detach()            # 2. 归一化            x = (x - self.mean) / self.stdev            # 3. 仿射变换 (可选)            if self.affine:                x = x * self.affine_weight + self.affine_bias            return x        elif mode == 'denorm':            if self.affine:                x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.affine_weight)            x = x * self.stdev + self.mean            return x        else:            return xclass PGFA(nn.Module):    """    [物理层修正] 物理引导频域注意力    """    def __init__(self, freq_dim: int, max_freq: float = 512.0, init_sigma: float = 2.0):        super().__init__()        self.sigma = nn.Parameter(torch.tensor(init_sigma))        self.alpha = nn.Parameter(torch.tensor(0.5))        self.register_buffer('freq_axis', torch.linspace(0, max_freq, freq_dim).view(1, -1, 1))    def forward(self, seasonal: torch.Tensor, speed_hz: torch.Tensor) -> torch.Tensor:        s = speed_hz.view(-1, 1, 1)        f = self.freq_axis        sig = torch.clamp(self.sigma, min=0.5, max=10.0)        denom = 2 * (sig ** 2)        # 1. 机械谐波 (1x, 2x, 3x)        mask_harmonics = torch.exp(- (f - s) ** 2 / denom) + \                         torch.exp(- (f - 2 * s) ** 2 / denom) + \                         torch.exp(- (f - 3 * s) ** 2 / denom)        # 2. 0.5x 次谐波 (松动/摩擦)        mask_sub = torch.exp(- (f - 0.5 * s) ** 2 / denom) * 0.5        total_mask = mask_harmonics + mask_sub        return seasonal * (1 + self.alpha * total_mask)class PhysRDLinearCls(nn.Module):    def __init__(self, config, enable_pgfa=True, enable_mtl=True, enable_acoustic=True):        super().__init__()        self.enable_pgfa = enable_pgfa        self.enable_mtl = enable_mtl        self.enable_acoustic = enable_acoustic        # === 1. Micro Stream (1Hz Res) ===        # [CRITICAL FIX] 替换 BatchNorm 为 FreqRevIN (Instance Norm)        # 这将解决 0kg 下 Healthy 准确率为 0 的问题        self.revin_micro = FreqRevIN(config.FREQ_DIM, affine=False)        self.decomp = SeriesDecomp(kernel_size=25)        if self.enable_pgfa:            self.pgfa = PGFA(                config.FREQ_DIM,                max_freq=config.SAMPLE_RATE / 2,                init_sigma=config.PGFA_SIGMA            )        self.lin_trend = nn.Linear(config.FREQ_DIM, config.HIDDEN_DIM)        self.lin_sea = nn.Linear(config.FREQ_DIM, config.HIDDEN_DIM)        # === 2. Macro Stream (50Hz Res) ===        # [CRITICAL FIX] 同样替换 Macro 流        self.revin_macro = FreqRevIN(config.FREQ_DIM, affine=False)        self.lin_macro = nn.Linear(config.FREQ_DIM, config.HIDDEN_DIM)        # === 3. Acoustic ===        # 使用 config 中的动态维度 (15)        audio_dim = getattr(config, 'FEAT_DIM_AUDIO', 15)        if self.enable_acoustic:            self.ac_head = nn.Sequential(                nn.Linear(audio_dim, 64),                nn.ReLU(),                nn.Linear(64, config.HIDDEN_DIM)            )        # === Fusion ===        # Trend + Sea + Macro + (Acoustic)        fusion_dim = config.HIDDEN_DIM * 3 + (config.HIDDEN_DIM if self.enable_acoustic else 0)        self.cls_head = nn.Sequential(            nn.BatchNorm1d(fusion_dim),  # 这里的BN可以保留，因为Feature已经Normalized            nn.Linear(fusion_dim, config.HIDDEN_DIM),            nn.ReLU(),            nn.Dropout(0.5),            nn.Linear(config.HIDDEN_DIM, config.NUM_CLASSES)        )        # === Virtual Load Sensor (MTL) ===        if self.enable_mtl:            self.load_head = nn.Sequential(                nn.Linear(config.HIDDEN_DIM, 32),                nn.ReLU(),                nn.Linear(32, 1)            )    def forward(self, micro, macro, acoustic, speed):        # micro: [B, 512, 1]        # --- Micro Path ---        # RevIN 归一化 (Instance Level)        mic_norm = self.revin_micro(micro, 'norm')        sea, trend = self.decomp(mic_norm)        if self.enable_pgfa:            sea = self.pgfa(sea, speed)        f_trend = self.lin_trend(trend.squeeze(-1))        f_sea = self.lin_sea(sea.squeeze(-1))        # --- Macro Path ---        mac_norm = self.revin_macro(macro, 'norm')        f_macro = self.lin_macro(mac_norm.squeeze(-1))        # --- Acoustic Path ---        feats = [f_trend, f_sea, f_macro]        if self.enable_acoustic:            f_ac = self.ac_head(acoustic)            feats.append(f_ac)        # --- Fusion ---        fusion = torch.cat(feats, dim=1)        logits = self.cls_head(fusion)        # --- MTL ---        pred_load = None        if self.enable_mtl:            pred_load = self.load_head(f_trend)        return logits, pred_load